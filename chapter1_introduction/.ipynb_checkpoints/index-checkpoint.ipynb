{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24c8006",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 引言\n",
    "\n",
    "机器学习（machine learning，ML）是一类强大的可以从经验中学习的技术。 通常采用观测数据或与环境交互的形式，机器学习算法会积累更多的经验，其性能也会逐步提高。\n",
    "\n",
    "![一个典型的训练过程](../img/ml-loop.svg)\n",
    "\n",
    "## 1.2 机器学习中的关键组件\n",
    "\n",
    "1. 可以用来学习的*数据*（data）；\n",
    "1. 如何转换数据的*模型*（model）；\n",
    "1. 一个*目标函数*（objective function），用来量化模型的有效性；\n",
    "1. 调整模型参数以优化目标函数的*算法*（algorithm）。\n",
    "\n",
    "\n",
    "## 1.3 各种机器学习问题\n",
    "\n",
    "### 监督学习\n",
    "\n",
    "监督学习（supervised learning）擅长在“给定输入特征”的情况下预测标签。 每个“特征-标签”对都称为一个样本（example）。 有时，即使标签是未知的，样本也可以指代输入特征。 我们的目标是生成一个模型，能够将任何输入特征映射到标签（即预测）。\n",
    "\n",
    "监督学习的学习过程一般可以分为三大步骤：\n",
    "1. 从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。有时，这些样本已有标签（例如，患者是否在下一年内康复？）；有时，这些样本可能需要被人工标记（例如，图像分类）。这些输入和相应的标签一起构成了训练数据集；\n",
    "2. 选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”；\n",
    "3. 将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测。\n",
    "\n",
    "整个监督学习过程如下图所示。\n",
    "\n",
    "![监督学习](../img/supervised-learning.svg)\n",
    "\n",
    "监督学习的类型：\n",
    "- 回归（regression）：输出为数值，常见损失为平方误差。\n",
    "- 分类（classification）：顾名思义，常见损失为交叉熵。\n",
    "- 标注问题：类似分类问题，但是还有多标签分类。\n",
    "- 搜索：不仅希望输出一个类别或者一个实值，还希望对一组项目进行排序。\n",
    "- 推荐系统：向特定用户进行“个性化”推荐。\n",
    "- 序列学习：输入和输出都是可变长度的序列。\n",
    "  - 标记和解析（用属性（比如词性）注释文本序列\n",
    "  - 自动语音识别\n",
    "  - 文本到语言\n",
    "  - 机器翻译\n",
    "\n",
    "\n",
    "### 无监督学习\n",
    "\n",
    "监督学习中，需要向模型提供巨大数据集，每个样本包含特征和相应标签值。而在无监督学习中，数据中并不含有“目标”。\n",
    "- 聚类（clustering）问题\n",
    "- 主成分分析（principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。\n",
    "- 因果关系（causality）和概率图模型（probabilistic graphical models）问题：我们能否描述观察到的数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？\n",
    "- 生成对抗性网络（generative adversarial networks）：为我们提供一种合成数据的方法。\n",
    "\n",
    "\n",
    "### 与环境互动\n",
    "\n",
    "到目前为止，不管是监督学习还是无监督学习，我们都会预先获取大量数据，然后启动模型，不再与环境交互。 这里所有学习都是在算法与环境断开后进行的，被称为离线学习（offline learning）。 对于监督学习，从环境中收集数据的过程类似于下图。\n",
    "\n",
    "![从环境中为监督学习收集数据。](../img/data-collection.svg)\n",
    "\n",
    "离线学习的好处是，我们可以孤立地进行模式识别，而不必分心于其他问题。 但缺点是，解决的问题相当有限。 这时我们可能会期望人工智能不仅能够做出预测，而且能够与真实环境互动。 与预测不同，“与真实环境互动”实际上会影响环境。 这里的人工智能变成了“智能代理”，而不仅仅是一个简单的“预测模型”——只产生一个输出。 因此，我们必须考虑到它的行为可能会影响未来的观察结果。\n",
    "\n",
    "考虑“与真实环境互动”将打开一整套新的建模问题。以下只是几个例子。\n",
    "\n",
    "* 环境还记得我们以前做过什么吗？\n",
    "* 环境是否有助于我们建模？例如，用户将文本读入语音识别器。\n",
    "* 环境是否想要打败模型？例如，一个对抗性的设置，如垃圾邮件过滤或玩游戏？\n",
    "* 环境是否重要？\n",
    "* 环境是否变化？例如，未来的数据是否总是与过去相似，还是随着时间的推移会发生变化？是自然变化还是响应我们的自动化工具而发生变化？\n",
    "\n",
    "当训练和测试数据不同时，最后一个问题提出了*分布偏移*（distribution shift）的问题。\n",
    "接下来的内容将简要描述强化学习问题，这是一类明确考虑与环境交互的问题。\n",
    "\n",
    "\n",
    "### 强化学习\n",
    "\n",
    "在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体从环境接收一些观察（observation），并且必须选择一个动作（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得奖励（reward）。 此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。 强化学习的过程在下图中进行了说明。\n",
    "\n",
    "![强化学习和环境之间的相互作用](../img/rl-environment.svg)\n",
    "\n",
    "可以将任何监督学习问题转化为强化学习问题。当然，强化学习还可以解决许多监督学习无法解决的问题。\n",
    "\n",
    "例如，在监督学习中，我们总是希望输入与正确的标签相关联。 但在强化学习中，我们**并不假设环境告诉智能体每个观测的最优动作**，智能体只是得到一些奖励，证明这个观测是好的还是坏的（具体是否最优并不清楚）。\n",
    "\n",
    "此外，**环境甚至可能不会告诉是哪些行为导致了奖励**。\n",
    "- 以强化学习在国际象棋的应用为例。 唯一真正的奖励信号出现在游戏结束时：当智能体获胜时，智能体可以得到奖励 1；当智能体失败时，智能体将得到奖励 -1。 因此，强化学习者必须处理学分分配（credit assignment）问题：决定哪些行为是值得奖励的，哪些行为是需要惩罚的。 \n",
    "- 就像一个员工升职一样，这次升职很可能反映了前一年的大量的行动。 要想在未来获得更多的晋升，就需要弄清楚这一过程中哪些行为导致了晋升。\n",
    "\n",
    "强化学习可能还必须处理**部分可观测性问题**。 也就是说，当前的观察结果可能无法阐述有关当前状态的所有信息。 \n",
    "- 比方说，一个清洁机器人发现自己被困在一个许多相同的壁橱的房子里。 推断机器人的精确位置（从而推断其状态），需要在进入壁橱之前考虑它之前的观察结果。\n",
    "\n",
    "最后，在任何时间点上，强化学习智能体可能知道一个好的策略，但可能有许多更好的策略从未尝试过的。 强化学习智能体必须不断地做出选择：**是应该利用当前最好的策略，还是探索新的策略空间（放弃一些短期回报来换取知识）**。\n",
    "\n",
    "一般的强化学习问题是一个非常普遍的问题。 智能体的动作会影响后续的观察，而奖励只与所选的动作相对应。 环境可以是完整观察到的，也可以是部分观察到的，解释所有这些复杂性可能会对研究人员要求太高。 此外，并不是每个实际问题都表现出所有这些复杂性。 因此，学者们研究了一些特殊情况下的强化学习问题。\n",
    "如下为一些特殊情况的强化学习问题：\n",
    "\n",
    "![强化学习问题](../img/强化学习问题.png)\n",
    "\n",
    "上图：多臂赌博机问题中，只有行动影响回报。中图：上下文赌博机问题中，状态和行动都影响回报。下图：完备强化学习问题中，行动影响状态，回报可能在时间上延迟。\n",
    "\n",
    "- 当环境可被完全观察到时，强化学习问题被称为**马尔可夫决策过程（markov decision process）**。\n",
    "- 当状态不依赖于之前的操作时，我们称该问题为**上下文赌博机（contextual bandit problem）**。 \n",
    "- 当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的**多臂赌博机（multi-armed bandit problem）**。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
